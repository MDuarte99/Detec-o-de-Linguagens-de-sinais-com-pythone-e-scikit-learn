# ğŸ–ï¸ Reconhecimento de Letras em Libras com VisÃ£o Computacional  

Este repositÃ³rio contÃ©m um projeto baseado no vÃ­deo de [Phillip](https://www.youtube.com/watch?v=MJCSjXepaAM), que utiliza visÃ£o computacional para capturar, processar e classificar gestos de letras em Libras (LÃ­ngua Brasileira de Sinais) usando a biblioteca MediaPipe e um classificador Random Forest.  

## ğŸ“Œ Funcionalidades  

âœ… **Coleta de Dados** â€“ Captura imagens de gestos de mÃ£o e cria um dataset.  
âœ… **Processamento de MÃ£os** â€“ Usa MediaPipe para detectar e extrair landmarks da mÃ£o.  
âœ… **Treinamento de Modelo** â€“ Treina um classificador Random Forest para reconhecer gestos.  
âœ… **InferÃªncia em Tempo Real** â€“ Detecta e exibe letras em Libras via webcam.  

## ğŸ› ï¸ Tecnologias Utilizadas  

- OpenCV  
- MediaPipe  
- Scikit-Learn  
- Matplotlib  
- Pickle  

## ğŸ“‚ Estrutura do Projeto  

ğŸ“ `Libras.ipynb` â†’ Captura e armazena imagens para o dataset.  
ğŸ“ `Cria_dataset.ipynb` â†’ Processa imagens e extrai landmarks das mÃ£os.  
ğŸ“ `treina_classificador.ipynb` â†’ Treina o modelo de classificaÃ§Ã£o.  
ğŸ“ `classificaÃ§Ã£o_por_inferencia.ipynb` â†’ Realiza a inferÃªncia em tempo real.  

## ğŸš€ Como Usar  

1. **Captura de Dados:**
   - Execute o script `Libras.ipynb` para capturar imagens dos gestos que deseja classificar.
   
2. **Processamento de Imagens:**
   - Execute o script `Cria_dataset.ipynb` para extrair os dados das mÃ£os e salvar em um arquivo `.pickle`.

3. **Treinamento do Modelo:**
   - Execute o script `treina_classificador.ipynb` para treinar o modelo de classificaÃ§Ã£o e salvar o modelo treinado.

4. **ClassificaÃ§Ã£o em Tempo Real:**
   - Execute o script `classificaÃ§Ã£o_por_inferencia.ipynb` para classificar gestos em tempo real usando a webcam.

## Requisitos

- Python 3.x
- Bibliotecas: OpenCV, MediaPipe, Scikit-learn, NumPy, Matplotlib, Pickle

## ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir issues ou pull requests para melhorias no cÃ³digo, documentaÃ§Ã£o ou novas funcionalidades.

## ReferÃªncias

- VÃ­deo de Philip: [https://www.youtube.com/watch?v=MJCSjXepaAM](https://www.youtube.com/watch?v=MJCSjXepaAM)
- DocumentaÃ§Ã£o do MediaPipe: [https://google.github.io/mediapipe/](https://google.github.io/mediapipe/)
- DocumentaÃ§Ã£o do OpenCV: [https://opencv.org/](https://opencv.org/)

Este repositÃ³rio Ã© uma implementaÃ§Ã£o prÃ¡tica de visÃ£o computacional e aprendizado de mÃ¡quina para reconhecimento de gestos, com foco na acessibilidade e inclusÃ£o atravÃ©s da Libras.
